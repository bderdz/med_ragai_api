# ğŸ‘¨â€âš•ï¸ Medical Diagnosis RAG Assistant

**A Retrieval-Augmented Generation (RAG) based medical diagnosis assistant
designed to assist in medical diagnosis. The assistant
helps to provide medical diagnosis based on patient symptoms, age and gender**
using a combination of a local vector database (`ChromaDB`), a local reranking model,
and reasoning LLM Gemini model via API.

Project is built with Fast API as main backend part and a simple chat wrapper around it
for project showcase, only for the needs of the subject **IBM LLM-Basic**.

## ğŸ” Overview

**This project created for 2 university subjects:**

1. **Medical information systems:** Main API implementation of medical diagnosis assistant.
2. **IBM LLM-Basic:** A chat wrapper around the API with local LLM model for collecting patient information.

### ğŸ› ï¸ Tech Stack:

- **Package Manager:** [uv](https://docs.astral.sh/uv/)
- **Backend:** `FastAPI`
- **Chat Frontend:** `Gradio`
- **LLM Framework:** `LangChain`
- **Vector Store:** `ChromaDB`
- **LLM Models:**
    - **Main Reasoning:** `gemini-2.5-flash` (via API)
    - **Chat Agent:** `Qwen2.5-0.5B-Instruct` (Local)
    - **Embedding:** `all-MiniLM-L6-v2`
    - **Reranking:** `MiniLM-L6-v2`
- **Testing**: `Pytest`
- **Logging:** `logging`
- **Data Handling:** `Pandas`, `NumPy`, `Pydantic`

### ğŸ“š Disease Dataset:

The disease dataset used in this project is sourced from:
[A Structured Bangla Dataset of Disease-Symptom Associations](https://data.mendeley.com/datasets/rjgjh8hgrt/5)

ğŸ‘‰ **If you want to see preprocessing steps, check notebook in this repo:** [dataset processing notebook](https://github.com/bderdz/disease_icd_dataset)

### âš™ï¸ Architecture Overview:

1. **Backend API (FastAPI)** - Acts as the main entry point of the system. It **receives patient data (symptoms, age, gender)**,
   **validates input using Pydantic schemas**, and orchestrates the **diagnosis pipeline**.
2. **Retrieval (ChromaDB + Embeddings)** - Disease profiles are embedded using the `all-MiniLM-L6-v2` model and
   stored in a local ChromaDB vector database. Relevant disease candidates are retrieved based on **semantic similarity to patient symptoms**.
3. **Reranking (CrossEncoder)** - Retrieved documents are reranked using a local `MiniLM-L6-v2` CrossEncoder model
   to improve retrieval precision. **The top-K reranked documents are selected as context.**
4. **Reasoning (Gemini API)** - The final diagnosis is generated by the `gemini-2.5-flash` model via API, using the
   patient data and retrieved medical context.
5. **Chat Interface (Gradio)** - A lightweight chat wrapper using a local LLM `Qwen2.5-0.5B-Instruct` is used to **collect
   patient information** and **communicate with the API by using tool**.

### ğŸ”’ Guardrails:

Since the project interact with medical data and LLMs, security and safety are prioritized
using a dedicated Guardrails layer:

- **Prompt injection protection**
- **Sensitive data detection**
- **Links and profanity filtering**
- **Tool usage validation**
- **Structured output**

## ğŸš€ Running the project

1. **Clone the repository:**

```bash
git clone https://github.com/bderdz/med_rag_assistant.git
cd med_ragai_api
```

2. **Setup project environment:**

use UV project manager (**Recommended**):

```bash
# Install UV if you don't have it yet
pip install uv

# In project root directory run:
uv sync
```

activate the environment:

```bash
source .venv/bin/activate
```

or run files with uv directly:

```bash
uv run <file_name>.py
```

3. **Configure environment variables:**

**Create a `.env` file in the root directory** (based on `.env.example`) and set your
API key.

âš ï¸ **NOTE: It's recommended not to change default LLM and Embedding models to ensure better performance.**

```ini
GOOGLE_API_KEY = YOUR_API_KEY
# Add other variables from .env.example here if you want to change defaults
# ...
```

4. **Start the App:**

```bash
uv run uvicorn main:app --host 0.0.0.0 --port 8000
```

The API will be available at http://localhost:8000

## ğŸ”¬ Evaluation

1. **Recall@K** evaluation used to evaluate the performance of the retrieval and reranking.

`evaluate.py` script runs the evaluation with provided **Top-K and sample size parameters** on the
test set **with and without reranking**.
To run the evaluation, use the following command at the project root directory:

```bash
uv run evaluate.py
```

2. **LLM Models metrics** such as **latency and token usage** automatically logged into `logs/metrics.log` file during API usage.

## âœ… Tests

To run tests, use the following command at the project root directory:

```bash
# Default test run
uv run pytest
# Run tests and save results to a txt file
uv run pytest > logs/test_results.txt
```

*Tests logs will be saved in the `logs/pytest.log` file.*

1. API: `tests/test_api.py` - Tests for API, request validation and response structure.
2. Tool parser: `tests/test_parser.py` - Tests JSON parsing and error handling in the tool parser.
3. Dispatcher: `tests/test_dispatcher.py` - Tests timeouts, allowed tools validation and error handling in the dispatcher.
4. Guardrails: `tests/test_guardrails.py` - Tests for guardrails validation and error handling.